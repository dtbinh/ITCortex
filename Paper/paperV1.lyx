#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass svglobal3
\begin_preamble
\bibpunct{(}{)}{,}{a}{,}{,}
\usepackage{epstopdf} 
\end_preamble
\options twocolumn,final
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
A Phenomenological Model of the Macaque Inferior Temporal Cortex
\end_layout

\begin_layout Author
Salman Khan 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
and
\end_layout

\end_inset

 Bryan Tripp
\end_layout

\begin_layout Institute
Salman Khan 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
at
\end_layout

\end_inset

 System Design Engineering Department, University of Waterloo, Waterloo,
 Canada.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash

\backslash

\backslash
email{s362khan@uwaterloo.ca}
\end_layout

\end_inset

 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
and
\end_layout

\end_inset

 Bryan Tripp 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
at
\end_layout

\end_inset

 System Design Engineering Department, University of Waterloo, Waterloo,
 Canada 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash

\backslash

\backslash
email{btripp@uwaterloo.ca}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
keywords{Inferior temporal cortex 
\backslash
and Monkey visual system 
\backslash
and Tuning curves models 
\backslash
and Receptive fields 
\backslash
and Ground truth extraction}
\end_layout

\end_inset


\end_layout

\begin_layout Section*
TODO
\end_layout

\begin_layout Itemize
Institution address not displayed correctly - Should appear as a foot note
 next to each author.
\end_layout

\begin_layout Itemize
How to identify corresponding author.
\end_layout

\begin_layout Itemize
Figures are currently only shrunk, can use cropping to make slightly larger.
 (do at end).
 Furthermore different figures use different shrink percentages.
 How to align, and make okay for diffrent formats.
\end_layout

\begin_layout Itemize
Need to add a little more description on how Euler angles are converted
 to rotations about the y axis of the vision sensor.
\end_layout

\begin_layout Itemize
Results section should be independently readable.
 It should not rely upon having read the methods section.
 Verify this is possible.
\end_layout

\begin_layout Itemize
Similarly, figure annotations need to be readable to give a summary of the
 work done.
 Verify this is possible.
\end_layout

\begin_layout Itemize
Methods section on temporal profile needs to be completed.
\end_layout

\begin_layout Itemize
Results section for clutter & temporal profile if needed.
\end_layout

\begin_layout Itemize
Introduction and discussion sections need to be completed.
 Talking points are highlighted.
 However these need to be expanded and presented cohesively.
\end_layout

\begin_layout Itemize
Abstract section needs to be completed.
\end_layout

\begin_layout Section*
Abstract
\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard

\emph on
Invariant population level representation and transform sensitive individual
 neurons
\end_layout

\begin_layout Standard
Given the variety of transformation objects display in complex natural environme
nts and the variability in spike patterns of lower layer neurons, each IT
 experience of an object can be considered unique, 
\begin_inset CommandInset citation
LatexCommand citet
key "dicarlo2012"

\end_inset

.
 Yet, object identity has been shown to be decodable from population responses
 using simple biologically plausible classifiers, 
\begin_inset CommandInset citation
LatexCommand cite
key "hung2005"

\end_inset

.
 This remarkable capability of the IT cortex was thought to extend to individual
 IT neurons as well.
 However, responses of individual IT neurons were found to be strongly impacted
 by these transformations.
 Analogously, IT neurons may be concurrently encoding object attributes
 that are population level decodable.
 Infact, 
\begin_inset CommandInset citation
LatexCommand cite
key "hung2005"

\end_inset

 demonstrated that coarse position and scale information can be extracted
 from the activity of IT neurons.
\end_layout

\begin_layout Standard

\emph on
Purpose of encoding object attributes with object identity in IT population
 responses
\end_layout

\begin_layout Standard
If object attributes are decodable from the population activity of ventral
 visual stream IT neurons, how does it corelate with the spatial information
 extracted by the dorsal visual stream.
 Some have hypothesized that precise identities and coarse attributes from
 the ventral stream are used to link precise spatial attributes and coarse
 identities from the dorsal stream in some further upstream area.
 However, precise object attributes for some transformations such as clutter
 and occlusion may not be available from the dorsal pathway.
 For such transforms, the brain must use information contained only within
 the ventral stream.
 Alternatively, given the infinite number of possible object transformations,
 the encoding of object attributes may just be a result of the capacity
 limitation of individual neurons.
\end_layout

\begin_layout Standard

\emph on
Specific Objective
\end_layout

\begin_layout Standard
How the IT cortex mechanistically achieves complete object preception is
 still not understood.
 Clearly there is a need to develop a strong understanding of how these
 transformations influence IT neuronal responses at both the individual
 and population level.
 In this paper we develop a single integrated model that can replicate observed
 individual and population responses of the IT cortex given object ground
 truth (identity + attributes) from a scene.
 Specifically, we target: (1) object selectivity, (2) maximum fire rates,
 (3) position, (4) size, (5) orientation, (6) occlusion, (7) clutter and
 (8) dynamic responses of IT neurons.
 For each selected transformation, we find statistical fit(s) for observed
 tuning profiles.
 For each statistical model, we develop models for distributions of their
 parameters and reproduce realistic population level responses.
 Finally, we use Virtual Robot Experimentation Platform (V-REP) to generate
 a complex visual scene and develop techniques for extracting ground truth.
\end_layout

\begin_layout Section*
Methods 
\begin_inset CommandInset label
LatexCommand label
name "sec:Methods"

\end_inset


\end_layout

\begin_layout Standard
When selecting references to model tuning profiles, we looked for those
 that not only measured responses from a large set of IT neurons while covering
 a wide range of the targeted stimulus with fine resolution but also those
 that provide population level statistics that allow for modelling groups
 of IT neurons.
 Often, this required combining results from multiple references.
 Because the IT cortex deals with complex stimuli references frequently
 use different measures for stimulus quantification.
 For all such cases we show how we convert between the different measures.
 Moreover, references typically isolate a single stimulus dimension and
 present tuning profiles as stimuli change along the chosen dimension.
 In our integrated model, we assume multidimensional tuning curves are separable
 allowing them to be modelled in isolation.
\end_layout

\begin_layout Standard
Data from selected references was extracted using an online data extraction
 tool, Web plot digitizer (
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://arohatgi.info/WebPlotDigitizer/
\end_layout

\end_inset

).
 Best fits were found using either maximum likelihood (ML) or least square
 error (LSE) estimation.
 Typically, data fitting was used in three ways.
 First, to find distributions for model parameters for a specified tuning
 profile model.
 Second, if none existed, it was used to find a distribution that best described
 recorded neuronal activity.
 Third, it was used to validate that our chosen profile can describe multiple
 recorded tuning profiles with reasonable parameters.
 In the following sections we describe tuning profiles for the selected
 attributes.
 All profiles are assumed to be normalized to their maximum value and all
 measurements are in radians of visual space unless explicitly stated otherwise.
\end_layout

\begin_layout Subsection*
Tuning Profiles
\end_layout

\begin_layout Subsubsection*
Stimulus Selectivity
\end_layout

\begin_layout Standard
It is well established that IT neurons respond to stimuli more complex then
 those of lower layers 
\begin_inset CommandInset citation
LatexCommand citep
key "tanaka1996,desimone1984"

\end_inset

.
 In the literature, several different metrics have been used to characterize
 how IT neurons respond preferentially towards certain objects compared
 to others, their object selectivities.
 See 
\begin_inset CommandInset citation
LatexCommand citet
key "lehky2011,zoccolan2007"

\end_inset

 for a review.
 Similar to 
\begin_inset CommandInset citation
LatexCommand citet
key "lehky2011"

\end_inset

, we quantify object selectivity using excess Kurtosis, as they provide
 results over a large stimulus set.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
SI_{K}=\frac{\sum_{i=1}^{N}(r_{i}-\bar{r})^{4}}{Ns^{4}}-3
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $r_{i}$
\end_inset

 is the response of the neuron to the 
\begin_inset Formula $ith$
\end_inset

 stimulus, 
\begin_inset Formula $\bar{r}$
\end_inset

 is the mean response across all stimuli, 
\begin_inset Formula $s$
\end_inset

 is the standard deviation of responses and 
\begin_inset Formula $N$
\end_inset

 is the number stimuli.
 The Kurtosis of a distribution provides a measure of its peak sharpness
 and tail heaviness.
 The -3 term scales the Kurtosis to be in excess of that of a normal distributio
n.
 A distribution with a high Kurtosis generates large responses (peak sharpness)
 for a few object while most objects generate minimal respones (tail heaviness).
 A distribution with a low Kurtosis responds similarly to several objects
 (broad peaks) with no discernible responses for most (small tails).
\end_layout

\begin_layout Standard
As done by 
\begin_inset CommandInset citation
LatexCommand cite
key "lehky2011"

\end_inset

, neuronal selectivities over objects are modeled as Gamma distributions
 with differing shape and scale parameters.
 Distributions for the shape and scale parameters were also modeled as Gamma
 distributions.
 We used their shape parameter distribution but adjusted the scale parameter
 distribution to account for non-optimal positions and sizes of their test
 stimuli.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "lehky2011"

\end_inset

, all stimuli were presented foveally.
 However the receptive field centers of IT neurons, their optimal positions,
 are distributed around the fovea.
 Additionally, all stimuli were of a fixed size and maximally extended 7
\begin_inset Formula $\textdegree$
\end_inset

 degrees across the visual field.
 To account for this we created a large sample of position and size profiles
 and determined the amount of deviation expected if stimuli were presented
 foveally with a fixed size of 7
\begin_inset Formula $\textdegree$
\end_inset

.
 The mean and variance of these deviations were used to adjust the parameters
 of the scale distribution.
 Once the selectivity profile of a neuron was determined, each stimulus
 object, 
\begin_inset Formula $m$
\end_inset

, was randomly assigned a value between [0, 1].
 The inverse of the cumulative frequency distribution (cdf) was used to
 quantify its preference for that object, 
\begin_inset Formula $pref_{obj_{m}}$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
.
\end_layout

\begin_layout Standard
A second measure of neuron selectivity, the activity fraction 
\begin_inset CommandInset citation
LatexCommand citep
key "zoccolan2007"

\end_inset

, was also calculated for each model neuron.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
SI_{AF}=\frac{(N-1)}{N}\left(1-\frac{\left(\sum\frac{r_{i}}{N}\right){}^{2}}{{\textstyle \sum}\frac{r_{i}^{2}}{N}}\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $r_{i}$
\end_inset

 is the response to the 
\begin_inset Formula $ith$
\end_inset

 stimulus and 
\begin_inset Formula $N$
\end_inset

 is the number of test stimuli.
 Activity fractions range between [0, 1].
 A neuron with an activity fraction of 0 responds equally to all stimuli
 (less selective) while a neuron with a selectivity index of 1 shows preference
 to a few stimuli only (highly selective).
 The activity fraction selectivity of a neuron was used to model decreases
 in tolerances as selectivity increases in several tuning profile, as observed
 in 
\begin_inset CommandInset citation
LatexCommand cite
key "zoccolan2007"

\end_inset

.
 
\end_layout

\begin_layout Subsubsection*
Maximum Firing Rate
\end_layout

\begin_layout Standard
Because modelled tuning profiles are normalized, the maximum firing rate
 of a model neuron needs to be specified to ascertain its raw firing rate.
 Distributions of absolute maximum firing rates are difficult to come by
 as they require measuring responses across an infinite range of objects
 and from a large number of neurons.
 Instead, we use the stimulus selectivity profile of a neuron to get its
 maximum firing rate.
 Using the inverse cdf of this distribution, we calculate the raw object
 preference for an object in the 99th percentile and use this as a measure
 of a neurons maximum firing rate.
 Additionally, once a population of IT neurons is created a common scale
 factor is used to adjust the firing rates of all neurons.
 Given the richness of the modelled selectivity distribution, this results
 in a wide range of maximum firing rates across the population.
 As has be observed in distributions of average firing rates of IT neurons,
 
\begin_inset CommandInset citation
LatexCommand citet
key "lehky2011"

\end_inset

.
\end_layout

\begin_layout Subsubsection*
Position Tuning
\end_layout

\begin_layout Standard
Typically, IT neurons perfer stimuli at a particular location within the
 visual field and their responses gradually decline as stimuli move away.
 
\begin_inset CommandInset citation
LatexCommand citet
key "op2000"

\end_inset

 found that spatial receptive fields of IT neurons are well approximated
 by 2D Gaussian distributions which we use to model their position tuning:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{multline}
r_{position}(x,y)=\exp\left(\frac{-\left((x-x_{c})^{2}+(y-y_{c})^{2}\right)}{2\sigma_{PT}}\right)
\end{multline}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $(x,\,y$
\end_inset

) is the position coordinates of the stimulus object, 
\begin_inset Formula $(x_{c},y_{c})$
\end_inset

 is the receptive field center - the neurons preferred stimulus position,
 and 
\begin_inset Formula $\sigma_{PT}$
\end_inset

, the position tolerance, is a measure of the neurons tolerance to deviations
 from its preferred position.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "op2000"

\end_inset

 found that preferred positions were distributed around the fovea with a
 slight bias towards the contralateral visual hemisphere.
 We model the x and y coordinates of the receptive field center as independent
 Gaussian distributions.
 Parameters for these distributions were found by least square error (LSE)
 fitting the receptive field center of masses in figure 6 of 
\begin_inset CommandInset citation
LatexCommand cite
key "op2000"

\end_inset

.
 
\end_layout

\begin_layout Standard
Position tolerance, the spatial extent of the receptive field is quantified
 as twice the standard deviation of the underlying Gaussian distribution
 
\begin_inset CommandInset citation
LatexCommand citep
key "zoccolan2007"

\end_inset

.
 Position tolerance in the x and y direction were found to be similar and
 we use a single parameter to model spatial extent in both directions.
 
\begin_inset CommandInset citation
LatexCommand cite
key "zoccolan2007"

\end_inset

 also observed that average position tolerance and variance from the preferred
 position both decreased with activity fraction selectivity.
 We use a gamma distribution to model the position tolerance distribution.
 To find the parameters of this distribution first we did a linear regression
 fit of the position tolerance versus neuron selectivity (activity fraction)
 scatter plot, figure 4a, of 
\begin_inset CommandInset citation
LatexCommand cite
key "zoccolan2007"

\end_inset

.
 This was used to model how mean position tolerance decreases with selectivity.
 Second, we assumed the shape parameter of the distribution was fixed and
 the scale parameter, derived from the mean, decreased with selectivity.
 Finally we used maximum likelihood estimation (MLE) to find the best fit
 shape and scale parameters subject to the above constraints.
\end_layout

\begin_layout Subsubsection*
Size Tuning
\end_layout

\begin_layout Standard
IT neurons generally perfer stimuli of a particular size and their responses
 drop with any changes.
 We model the size tuning of IT neurons using a lognormal distribution with
 parameters preferred size and size tolerance:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
r_{size}(s)=\exp\left(\frac{-\left(\log_{2}(s)-\log_{2}(\mu_{s})\right)^{2}}{2\sigma_{ST}^{2}}\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $s$
\end_inset

 is the size of the stimulus object, 
\begin_inset Formula $\mu_{s}$
\end_inset

 is the neurons preferred stimulus size, and 
\begin_inset Formula $\sigma_{ST}$
\end_inset

, the size tolerance, is a measure of the neurons tolerance to changes in
 size.
 Stimulus size, especially for complex stimuli that IT neurons respond to,
 can be quantified in several ways.
 As done in 
\begin_inset CommandInset citation
LatexCommand citet
key "ito1995"

\end_inset

, we use the maximum distance between the outer edges of the stimulus along
 any of its dimension.
\end_layout

\begin_layout Standard
Across their recorded population, 
\begin_inset CommandInset citation
LatexCommand cite
key "ito1995"

\end_inset

 found a wide range of preferred stimulus sizes.
 It was also found that preferred size does not depend on the size of the
 spatial receptive fields of neurons.
 Two peak preferred sizes of 3.4
\begin_inset Formula $\textdegree$
\end_inset

 and 27
\begin_inset Formula $\textdegree$
\end_inset

 degrees were seen, see figure 7B of 
\begin_inset CommandInset citation
LatexCommand citet
key "ito1995"

\end_inset

.
 While modeling the distribution of preferred stimulus size we ignored sizes
 larger than or equal to 27
\begin_inset Formula $\textdegree$
\end_inset

 degrees.
 These sizes were close to the largest stimulus and several neurons had
 receptive fields that could accommodate much larger stimuli.
 It is likely that their preferred stimulus sizes would be different if
 larger stimuli were used.
 This is not to say that preferred sizes greater than 27
\begin_inset Formula $\textdegree$
\end_inset

 degrees are not generated by our model but that the frequency of their
 occurrence follows the trends seen at lower sizes.
 We used MLE to fit the remaining data to a lognormal distribution to generate
 preferred stimulus sizes.
 Additionally, we restricted a model neurons preferred stimulus size to
 lie within its spatial receptive field.
 We define the spatial extent of a neurons receptive field using a circle
 with radius equal to its position tolerance and restrict the maximum stimulus
 size to be less than its diameter, 
\begin_inset Formula $\mu_{s_{max}}\leq2\sigma_{PT}$
\end_inset

.
\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset citation
LatexCommand citet
key "ito1995"

\end_inset

, tolerance to size changes, defined as size bandwidth, was quantified as
 the distance between the upper and lower half magnitude responses of a
 neurons size tuning profile in octaves.
 We use equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:size-bw-to-size-tolerance"

\end_inset

 to convert from full scale half magnitude size bandwidth, 
\begin_inset Formula $bw_{s}$
\end_inset

, to the standard deviation of our lognormal distribution.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\sigma_{ST}=\frac{bw_{s}}{2\sqrt{2\ln(2)}}\label{eq:size-bw-to-size-tolerance}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
A lognormal distribution was also used to model the distribution of size
 bandwidths.
 Parameters for this distribution were determined by MLE fitting the scatter
 plot of size bandwidths in figure 7A of 
\begin_inset CommandInset citation
LatexCommand cite
key "ito1995"

\end_inset

.
 A negative correlation between activity fraction selectivity and size tolerance
 was observed by 
\begin_inset CommandInset citation
LatexCommand cite
key "zoccolan2007"

\end_inset

.
 However given the limited range of sizes tested (1
\begin_inset Formula $\textdegree$
\end_inset

 , 2
\begin_inset Formula $\textdegree$
\end_inset

 , 4
\begin_inset Formula $\textdegree$
\end_inset

 , and 6
\begin_inset Formula $\textdegree$
\end_inset

 degrees), we do not include this observation in our model.
\end_layout

\begin_layout Subsubsection*
Orientation Tuning
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "logothetis1995"

\end_inset

 found that IT neurons typically prefer a particular view of an object -
 most familiar view - and their responses gradually declined as views rotate
 away.
 This preference persisted even as the monkey familiarized with the object
 and developed view invariant recognition at the behavioral level.
 We model orientation tuning of IT neurons using a Gaussian distribution
 with parameters preferred view and rotational tolerance.
 
\begin_inset CommandInset citation
LatexCommand citet
key "logothetis1995"

\end_inset

 also found several neurons with tuning curves centered around two views.
 In all such cases, peak views displayed some form of symmetry, including
 mirror symmetry.
 Test stimuli were either irregular wire or amoeboid objects that were never
 completely symmetric and peak amplitude differences were seen in these
 bimodal tuning curves.
 We do not model degree of symmetry between preferred views and use a binary
 metric to judge similarity - all symmetric views share the same peak amplitude.
 A few neurons also displayed view invariant responses to some stimuli.
 We assume multiple peaks in tuning curves are the result of symmetries
 in object views.
 From an individual neurons perspective, responses are based on the similarity
 to its preferred view rather than absolute orientation.
\end_layout

\begin_layout Standard
To model responses to symmetric objects, we assume each object has a symmetry
 frequency around the x, y, and z axis.
 Where the symmetry frequency defines the largest number of identical views
 within a 360
\begin_inset Formula $\textdegree$
\end_inset

 rotation.
 All views within the corresponding symmetry period are distinguishable
 while orientations outside are mapped within.
 Responses to mirror symmetric objects are modeled as the sum of two Gaussian
 distributions with the second Gaussian centered around the mirror symmetric
 preferred view.
 The complete rotation tuning profile of an IT neuron, over the 
\begin_inset Formula $(-\pi,\pi)$
\end_inset

 range, is stimulus dependent and is a mixture of Gaussians with equal tolerance
s centered around the neurons preferred view and any symmetric object views:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{multline}
r_{rotation}(\theta,\,f_{sym},\,m_{sym})=\\
\exp\left(\frac{-\left(\theta_{adj}-\mu_{r}\right)^{2}}{2\sigma_{RT}^{2}}\right)+\,m_{sym}\exp\left(\frac{-\left(\theta_{adj}-\mu_{r'}\right)^{2}}{2\sigma_{RT}^{2}}\right)
\end{multline}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\theta$
\end_inset

 is the absolute orientation of the stimulus object, 
\begin_inset Formula $\theta_{adj}$
\end_inset

 is 
\begin_inset Formula $\theta$
\end_inset

 adjusted to lie within the symmetry period of the object 
\begin_inset Formula $\frac{2\pi}{f_{sym}}$
\end_inset

, 
\begin_inset Formula $\mu_{r}$
\end_inset

 is the neurons preferred view adjusted to lie within the symmetry period,
 
\begin_inset Formula $\sigma_{RT}$
\end_inset

 is the rotational tolerance, 
\begin_inset Formula $m_{sym}\in\{0,\,1\}$
\end_inset

 determines whether the object is mirror symmetric, 
\begin_inset Formula $\mu_{r'}$
\end_inset

 is mirror reflection of the preferred view adjusted to lie within the symmetry
 period.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "logothetis1995"

\end_inset

 observed a wide range of preferred views and surrounding tolerances across
 their recorded population.
 We model the distribution of preferred view as a uniform random variable
 over
\begin_inset Formula $(-\pi,\pi)$
\end_inset

.
 Rotational tolerances, defined as the standard deviation of the fitted
 Gaussian distributions, averaged approximately 30
\begin_inset Formula $\textdegree$
\end_inset

 and were found to be similar about the x and y axes.
 Absent any further details and to generate a wide range of rotational tolerance
s, we used a normal distribution with mean 30
\begin_inset Formula $\textdegree$
\end_inset

 and standard deviation of 50
\begin_inset Formula $\textdegree$
\end_inset

.
 We model tuning about the y-axis only and assume similar profiles about
 x and z axes but with independent preferred views, tolerances and symmetries,
 as observed by 
\begin_inset CommandInset citation
LatexCommand citet
key "logothetis1995"

\end_inset

.
\end_layout

\begin_layout Subsubsection*
Occlusion Tuning
\end_layout

\begin_layout Standard
Responses of IT neurons increase monotonically with object visibility, 
\begin_inset CommandInset citation
LatexCommand cite
key "kovacs1995"

\end_inset

.
 IT neurons respond to object parts rather than the whole and prefer certain
 parts over others.
 These diagnostic parts are task dependant and behaviorally relevant to
 the attended task, 
\begin_inset CommandInset citation
LatexCommand citet
key "gosselin2001"

\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand cite
key "nielsen2006"

\end_inset

 found that individual IT neurons varied in their degree of preference for
 diagnostic parts.
 However diagnostic parts were always preferred over nondiagnostic parts.
 In their experiments, trials were grouped as either diagnostic or nondiagnostic
, depending on which parts were visible, and the amount of total variance
 explained by the between group variance was used to measure a neurons preferenc
e for diagnostic parts, 
\begin_inset Formula $R$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
R=\frac{\sigma{}_{b}}{\sigma{}_{t}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\sigma{}_{b}=\frac{1}{2}\left((\bar{f}_{d}-\bar{f})^{2}+(\bar{f}_{nd}-\bar{f})^{2}\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $R$
\end_inset

 ranges between 
\begin_inset Formula $[0,\,1]$
\end_inset

, 
\begin_inset Formula $\sigma{}_{b}$
\end_inset

 is the variance between diagnostic and nondiagnostic groups, 
\begin_inset Formula $\sigma_{t}$
\end_inset

 is the variance across all trials, 
\begin_inset Formula $\bar{f}$
\end_inset

 is the mean firing rate across all trials, 
\begin_inset Formula $\bar{f}_{d}$
\end_inset

 is the mean firing rate across trials where only diagnostic parts were
 visible and 
\begin_inset Formula $\bar{f}_{nd}$
\end_inset

 is the mean firing rate across nondiagnostic trials.
 A ratio of 1 indicates a high between groups variance, group mean firing
 rates are significantly different from the overall mean, and the neuron
 has a strong preference for diagnostic parts.
 While a ratio of 0 indicates that there is no difference between group
 variances, group mean firing rates all similar and equivalent to the overall
 mean, and the neuron responds equally to all parts.
\end_layout

\begin_layout Standard
We model the occlusion tuning profile of IT neurons using a two input logistic
 sigmoid function
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
r_{occlusion}(\mathbf{v})=\sigma(\mathbf{v.w}+b)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $\mathbf{v}=[v_{nd},\,v_{d}]^{T}$
\end_inset

 is a vector of nondiagnostic and diagnostic visibilities, 
\begin_inset Formula $\mathbf{w}=[w_{nd},\,w_{d}]^{T}$
\end_inset

 is a vector of diagnostic and nondiagnostic weights, 
\begin_inset Formula $b$
\end_inset

 is a bias term and 
\begin_inset Formula $\sigma(x)=\frac{1}{1+e^{-x}}$
\end_inset

 is the logistic function.
 
\end_layout

\begin_layout Standard
To determine a neurons 
\begin_inset Formula $w_{d}$
\end_inset

, 
\begin_inset Formula $w_{nd}$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 parameters, first its 
\begin_inset Formula $R$
\end_inset

 value is selected.
 The distribution of 
\begin_inset Formula $R$
\end_inset

 was modeled using data extracted from figure 4 of 
\begin_inset CommandInset citation
LatexCommand citet
key "nielsen2006"

\end_inset

.
 LSE fitting was used to fit an exponential distribution and its inverse
 cdf was used to find 
\begin_inset Formula $R$
\end_inset

.
 This is still underconstraint to generate 
\begin_inset Formula $w_{d}$
\end_inset

 and 
\begin_inset Formula $w_{nd}$
\end_inset

 as there are no bounds on their ranges nor any relation that can be exploited.
 To overcome this, we looked at occlusion tuning curves of IT neurons from
 other references.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "kovacs1995"

\end_inset

 tuning curves for multiple neurons to several occluded objects were recorded.
 However, diagnostic and nondiagnostic visibilities were not separated and
 a single combined measure of visibility was used.
 We assumed at a given combined visibility level, 
\begin_inset Formula $v_{d}$
\end_inset

 and 
\begin_inset Formula $v_{nd}$
\end_inset

 were equal and used LSE fitting to fit single-input logistic functions
 with parameters combined visibility weight, 
\begin_inset Formula $w_{c}$
\end_inset

, and bias, 
\begin_inset Formula $b$
\end_inset

.
 The means and variances of 
\begin_inset Formula $w_{c}$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 across all fits were used to construct separate normal distributions.
 We assume weights are related by 
\begin_inset Formula $w_{c}=w_{d}+w_{nd}$
\end_inset

.
 Given 
\begin_inset Formula $w_{c}$
\end_inset

, 
\begin_inset Formula $b$
\end_inset

 and 
\begin_inset Formula $R$
\end_inset

 of a neuron, we used the Powell Hybrid method of numerical root finding
 of nonlinear equations to find a set of 
\begin_inset Formula $w_{d}$
\end_inset

 and 
\begin_inset Formula $w_{nd}$
\end_inset

 that can generate the desired 
\begin_inset Formula $R$
\end_inset

.
 This was done using the fsolve function from Scipy's Optimize library with
 initial estimates for 
\begin_inset Formula $w_{d}$
\end_inset

 and 
\begin_inset Formula $w_{nd}$
\end_inset

 set at 
\begin_inset Formula $w_{c}/2$
\end_inset

.
\end_layout

\begin_layout Standard
Responses of IT neurons are also known to take longer to develop when objects
 are occluded 
\begin_inset CommandInset citation
LatexCommand cite
key "kovacs1995,nielsen2006"

\end_inset

.
 Currently this additional latency is not included in our model.
\end_layout

\begin_layout Subsubsection*
Clutter Tuning
\end_layout

\begin_layout Standard
Responses of IT neurons are known to be influenced by collocated objects.
 Response attenuation has also be observed from objects which individually
 do not elicit responses.
 Moreover, IT responses are also impacted by background complexity, 
\begin_inset CommandInset citation
LatexCommand citet
key "rolls2003"

\end_inset

.
 Under limited clutter conditions, two to three objects on top a plain backgroun
d, 
\begin_inset CommandInset citation
LatexCommand cite
key "zoccolan2005"

\end_inset

 showed that responses of individual IT neurons to multiple objects were
 smaller than the linear sum of their isolated responses and were infact
 closer to the average of isolated responses of constituent objects.
 Multiple averaging rules were tried in 
\begin_inset CommandInset citation
LatexCommand cite
key "li2009"

\end_inset

 and it was consistently found that averaging models better fit recorded
 responses compared with models base on the linear sum.
 
\end_layout

\begin_layout Standard
To model the clutter response of a model neuron, first its isolated responses
 to all objects are calculated
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{multline}
r_{isolated}(obj_{m})=R_{max}*pref_{obj_{m}}*r_{position}(x,y)\\
*r_{size}(s)*r_{rotation}(\theta,f_{sym},m_{sym})*r_{occlusion}(\mathbf{v})
\end{multline}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $x,\,y,\,s,\,\theta,\,f_{sym},\,m_{sym},\,v_{nd,}\,v_{d}$
\end_inset

 is the ground truth for object 
\begin_inset Formula $m$
\end_inset

 and 
\begin_inset Formula $R_{max}$
\end_inset

 is the neurons maximum firing rate.
 Inputs and parameters of individual profiles have been defined previously.
 Next a single clutter response is found by weighting isolated responses
 with their normalized position responses
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{multline}
r_{clutter}(obj_{1},\ldots,obj_{M})=\\
\frac{\sum_{m}^{M}r_{position\_obj_{m}}(x,\,y)*r_{isolated}(obj_{m})}{\sum_{m}^{M}r_{position\_obj_{m}}(x,\,y)}+d
\end{multline}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $M$
\end_inset

 is the number of visible objects and 
\begin_inset Formula $d$
\end_inset

 models the observed deviations from the averaging rule.
 To model the distribution of 
\begin_inset Formula $d$
\end_inset

, we used the scatter plots of joint responses plotted against the sum of
 isolated responses, figure 3, of 
\begin_inset CommandInset citation
LatexCommand cite
key "zoccolan2005"

\end_inset

 and recorded deviations from the averaging rule.
 Here our objective was not to model the data exactly but to get an estimate
 of the spread of the deviations from the expected average.
 A normal distribution with mean 0 and the measured standard deviation was
 used to model 
\begin_inset Formula $d$
\end_inset

.
 By weighting isolated responses with their position weights, clutter responses
 are impacted by any object within their receptive field regardless of whether
 the neuron is selective for that object in isolation or not, consistent
 with results in the literature.
\end_layout

\begin_layout Standard
We do not model the impact of background on clutter responses.

\emph on
 
\emph default
In 
\begin_inset CommandInset citation
LatexCommand cite
key "zoccolan2007"

\end_inset

, it was found that the clutter responses of IT neurons also decrease with
 activity fraction selectivity however this observation is also not included
 in our model.
\end_layout

\begin_layout Subsubsection*
Temporal Tuning
\end_layout

\begin_layout Subsection*
Model Description
\end_layout

\begin_layout Subsubsection*
Software architecture
\end_layout

\begin_layout Standard
Our simulation setup is shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:It-cortex-model"

\end_inset

.
 Functionally it is divided into two main parts.
 The first part handles the extraction of object ground truth (object identities
 and their attributes) from VREP scenarios.
 The second parts consists of population of neurons sensitive to objects
 in the VREP scenario.
 We refer to the first part as ground truth generation and the second part
 as our main IT cortex model.
 Raw ground truth from the VREP scenario is first processed into units and
 formats expected by model neurons.
 Individual neurons then calculate their isolated responses to each object.
 Using their clutter profiles, a single static response to all objects in
 their receptive fields is found.
 Finally, each model neurons outputs a single net response by filtering
 static responses through its temporal profile.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide true
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename UpdatedFigures/IT_cortex_model.eps
	scale 55

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Overall setup.
 (a) Software architecture of IT Cortex model.
 (b) VREP simulation of a typical kitchen.
 Dashed rectangle shows the IT cortex robot and the ground truth extracting
 vision sensor.
 The IT cortex robot is configured to move across the scene and exit through
 the door.
 Its vision sensor is rotated 90
\begin_inset Formula $\textdegree$
\end_inset

 to the direction of motion such that several objects fall within its field
 of vision.
 Inset shows the projection plane of the vision sensor.
 (c) Net firing rates of model neurons plotted over time.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:It-cortex-model"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The IT cortex model is designed to work with any VREP scene.
 Ground truth can be extracted automatically by simply placing our ground
 truth extracting Pioneer p3dx robot with the attached vision sensor into
 a scene.
 Although, to realize the full capacity of our model requires annotating
 objects with their rotational symmetries and labeling diagnostic parts
 prior to simulation execution.
 Model neurons are also highly configurable and can easily turn off individual
 tuning profiles without impacting the execution the rest of the program.
 The individual neuron then looses all sensitivity to that particular object
 attribute but can still work with the same input ground truth stream.
 Our simulation setup is written almost entirely in python and is freely
 available at 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://github.com/salkhan23/ITCortex
\end_layout

\end_inset

.
 Extracting visibilities levels required us to attached a custom VREP child
 script to our vision sensor.
 This was written in Lua as required by VREP.
\end_layout

\begin_layout Subsubsection*
Ground Truth Generation 
\end_layout

\begin_layout Standard
As inputs, the IT model expects a stream of objects and their attributes,
 object ground truth, from environments it is exploring.
 Finding a data set with annotated ground truth or extracting ground truth
 from videos is a nontrivial task especially considering some of the less
 familiar object attributes used in our model - visibilities of diagnostic
 and nondiagnostic parts.
 It is easier to consider a controlled environment were object identities
 and attributes can be precisely and readily extracted.
 We used Virtual Robot Experimentation Platform (VREP), 
\begin_inset CommandInset citation
LatexCommand cite
key "vrep"

\end_inset

, to create such a test scenario, see figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:It-cortex-model"

\end_inset

.
 A vision sensor is used to explore the environment and its sequence of
 2D renderings serves as the input stream for ground truth extraction.
 Rather than use some of VREPs built in object detection algorithms, a camera
 plane projection matrix is used to determine objects that lie on the vision
 sensors projection plane.
 This reduces computational complexity and increases accuracy.
 To allow convenient control of the vision sensors movement, we attached
 it to a Pioneer p3dx robot model, included with VREP.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="7" columns="3">
<features rotate="0" booktabs="true" tabularvalignment="middle">
<column alignment="left" valignment="top" width="2.75cm">
<column alignment="left" valignment="top" width="1.25cm">
<column alignment="left" valignment="top" width="3cm">
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Property
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Setting
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Notes
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Resolution X/Y
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
128/128
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Aspect ratio 
\begin_inset Formula $ar=1$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Perspective angle (
\begin_inset Formula $\alpha$
\end_inset

)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
90
\begin_inset Formula $\textdegree$
\end_inset

 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Near clipping 
\begin_inset Formula $(z_{n})$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.001 m
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Far clipping 
\begin_inset Formula $(z_{f})$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2m
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Render Mode
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
OpenGL, color coded handles
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Pixel values multiplexed with owning object handle.
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Explicit handling
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
True
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Allows for external control of vision sensors image capture.
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Properties of the VREP perspective projection-type vision sensor.
 
\begin_inset CommandInset label
LatexCommand label
name "tab:Properties-of-the-vision-sensor"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Simulation execution was controlled by the IT cortex model - synchronous
 operation mode - with a default timestep of 5ms.
 At initialization, the scenario is queried for a list of objects.
 Constant object attributes are also determined.
 This include the size, defined as twice the maximum length in the x, y
 or z dimension, the rotational symmetries, and VREP handles of all objects
 and their component parts.
 Object rotational symmetries are not a defined attribute of VREP objects
 and were manually added for all objects displaying any form of symmetry
 under the custom data field when the scene was created.
 Similarly, at scene creation, a 'diagnostic' label was appended to the
 names of diagnostic object parts.
 We assume diagnostic parts are known apriori and do not discuss how to
 identify them.
 See 
\begin_inset CommandInset citation
LatexCommand citet
key "gosselin2001"

\end_inset

 for a commonly used method.
 Finally, a user configurable population of IT neurons selective for these
 objects is created.
 Firing rates across the population are all scaled such that the component
 neuron with the hihest maximum firing rate fires at 100 spikes/s.
\end_layout

\begin_layout Standard
Each timestep the IT cortex model triggers the simulation to run and uses
 remote APIs to query for ground truth.
 We assume that the fovea is located at the center of the projection plane.
 To get projection plane coordinates of each object 
\begin_inset Formula $(x_{p},\,y_{p})$
\end_inset

, first, world coordinates of the objects center of mass in the reference
 frame of the vision center 
\begin_inset Formula $(x,\,y,\,z)$
\end_inset

 are retrieved.
 Where the reference frame of the vision system is a coordinate system with
 its origin at the vision sensors center of mass.
 Second, homogeneous coordinates are derived using:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{multline}
\left[\begin{array}{c}
x_{h_{p}}\\
y_{h_{p}}\\
z_{h_{p}}\\
w_{h_{p}}
\end{array}\right]=\left[\begin{array}{cccc}
\frac{1}{ar*tan(\frac{\alpha}{2})} & 0 & 0 & 0\\
0 & \frac{1}{tan(\frac{\alpha}{2})} & 0 & 0\\
0 & 0 & \frac{-(z_{n}+z_{f})}{(z_{n}-z_{f})} & \frac{2z_{n}z_{f}}{(z_{n}-z_{f})}\\
0 & 0 & 1 & 0
\end{array}\right]\left[\begin{array}{c}
x\\
y\\
z\\
1
\end{array}\right]\label{eq:homogenous-coordinates-conversion}
\end{multline}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $ar=\frac{screen\,width}{screen\,height}$
\end_inset

 is the aspect ratio of the projection plane, 
\begin_inset Formula $\alpha$
\end_inset

 is the perspective angle, 
\begin_inset Formula $z_{n},z_{f}$
\end_inset

 are the near and fear clipping distances respectively, and 
\begin_inset Formula $w_{h_{p}}$
\end_inset

 stores the original value of z through the transformation.
 
\begin_inset Formula $ar,\,\alpha,\,z_{n},\,z_{f}$
\end_inset

 are adjustable parameters of the vision sensor.
 Defaults used are shown in table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Properties-of-the-vision-sensor"

\end_inset

.
 Elements of this projection plane matrix are scaled to allow rapid detection
 of objects that lie in the projection plane - only objects with all 
\begin_inset Formula $(x_{h_{p}},\,y_{h_{p}},\,z_{h_{p}})$
\end_inset

 within the range (-1, 1) are visible in the projection plane.
 Finally, after confirming that an object exists in the projection plane,
 its coordinates are calculated:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\begin{array}{c}
x_{p}=\frac{x_{h_{p}}}{w_{h_{p}}}*ar*\frac{\pi}{2}\\
y_{p}=\frac{y{}_{h_{p}}}{w_{h_{p}}}*\frac{\pi}{2}
\end{array}\label{eq:homegenous-to-projection-plane}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where multiplying 
\begin_inset Formula $x_{h_{p}}$
\end_inset

with 
\begin_inset Formula $ar$
\end_inset

 normalizes the range of 
\begin_inset Formula $x_{p}$
\end_inset

 to lie within 
\begin_inset Formula $(-ar,\,ar)$
\end_inset

 and restores the aspect ratio of the projection plane, and the 
\begin_inset Formula $\frac{\pi}{2}$
\end_inset

 multiplier scales x to horizontal extent of the visual space.
\end_layout

\begin_layout Standard
The projection plane size of an object is calculated using:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
s{}_{p}=\frac{\pi*s}{2*ar*z_{e}*tan\left(\frac{\alpha}{2}\right)}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $s$
\end_inset

 is real world object size, 
\begin_inset Formula $z_{e}$
\end_inset

 the Euclidean distance of the object from the vision sensor, and 
\begin_inset Formula $\frac{\pi}{2}$
\end_inset

 re-scales to the same units as the position coordinates.
 
\end_layout

\begin_layout Standard
The orientation of an object is found by querying its Euler rotation angles
 (
\begin_inset Formula $\alpha,\,\beta,\,\gamma$
\end_inset

) with respect to the vision sensors reference frame.
 
\end_layout

\begin_layout Standard
To calculate object visibility levels, a custom VREP child script was added
 to the vision sensor.
 The IT cortex model communicated with the vision sensor using a dedicated
 signalling channel.
 Every timestep, a list of all projection plane objects is passed to the
 script.
 Individual list entries consisted of handles of a single object and all
 its component parts.
 The vision sensor was configured to multiplex object handles into each
 pixel, see table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Properties-of-the-vision-sensor"

\end_inset

.
 The script retrieves the vision sensors image, demultiplexes object handles
 and counts the number of visible pixels for each object.
 Next, the total pixel count of each object is determined.
 This is done by marking all other objects as unrenderable and thereby invisible
 to the vision sensor, reacquiring the projection image and counting the
 total number of object pixels.
 Visibility levels were calculated as the ratio of visible pixels to total
 pixels.
 Diagnostic parts of an object were additionally treated as a separate object
 allowing the script to retrieve diagnostic parts visibilities separately.
 Finally, the script sends visibility levels for each requested object back
 to the IT cortex model.
\end_layout

\begin_layout Section*
Results 
\begin_inset CommandInset label
LatexCommand label
name "sec:Results"

\end_inset


\end_layout

\begin_layout Standard
A model IT neurons tuning profiles and sample net firing rates from our
 VREP simulation scenario are shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:IT-model-neuron"

\end_inset

.
 The selection of these profiles and how their parameters are distributed
 across a population are detailed in the 
\begin_inset CommandInset ref
LatexCommand nameref
reference "sec:Methods"

\end_inset

 section.
 In the remainder of this section we discuss how our model can quantitatively
 reproduce several other observed properties of IT neurons and populations
 of them.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename UpdatedFigures/tuning_profiles_model_neuron.eps
	width 100col%
	height 30pheight%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Tuning profiles of a sample IT model neuron.
 First row left, gamma distributed object selectivity.
 Objects are plotted in decreasing order of selectivity.
 First row right 2D Gaussian postion profile.
 Second row left Lognomal size profile.
 Second row right multi-Gaussian sum rotation tuning for a mirror symmetric
 object with a rotation frequency of one.
 Third row left two input logistic occlusion tuning profile.
 Third row right position weighted averaging clutter profile for two objects
 in the neurons receptive field.
 Fourth row left temporal response profile for the neurons most preferred
 object, at its preferred tuning, visible for half the simulated time.
 Fourth row right, neurons net firing rates for a sample run through VREP
 simulation.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:IT-model-neuron"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Stimulus Selectivity
\end_layout

\begin_layout Standard
Analogous to an individual neurons object selectivity, the responses of
 IT neurons to a particular object can also be characterized by excess Kurtosis.
 Termed population sparseness, it was found in 
\begin_inset CommandInset citation
LatexCommand cite
key "lehky2011"

\end_inset

 to be generally higher than average object selectivity.
 Hence objects typically generate large responses from a few neurons, while
 individually, neurons respond to several objects.
 We created a population of 674 IT neurons and a stimulus set of 806 objects,
 similar to the 
\begin_inset CommandInset citation
LatexCommand cite
key "lehky2011"

\end_inset

, and plotted the distributions of object selectivity and population spareness
 in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:neuronal-selectivity-and-population-sparsness"

\end_inset

.
 This observation can be seen in our modelled population, even with our
 adjusted selectivity distributions, see section 
\begin_inset CommandInset ref
LatexCommand nameref
reference "sec:Methods"

\end_inset

.
 As a result of our adjustment, population spareness is lower than what
 was observed by 
\begin_inset CommandInset citation
LatexCommand citet
key "lehky2011"

\end_inset

.
 However, the critical observation still holds.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename UpdatedFigures/neuron_selectivity_vs_population_sparesness.eps
	scale 15

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Neuron selectivity and population sparseness distributions.
 Top, individual neuronal object selectivity histogram.
 N is the number of neurons in the modeled population.
 Bottom, population sparseness histogram.
 N is the number of stimuli.
 On average, population sparseness is greater than individual neuronal selectivi
ties.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:neuronal-selectivity-and-population-sparsness"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Firing Rate
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "lehky2011"

\end_inset

 also found a strong negative correlation between an IT neurons object selectivi
ty and its average firing rate.
 To show that this is observable in our model, we created a similar sized
 population, measured responses over a similar sized stimulus set and plot
 object selectivities (defined by excess Kurtosis) against average firing
 rate in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:neuronal-selectivity-and-population-sparsness"

\end_inset

.
 For each model neuron we assumed objects were presented at its preferred
 tuning such that the only factor influencing output firing rates was object
 preferences.
 Firing rates were found by averaging responses across all objects in the
 stimulus set.
 As can be seen more selective neurons typically have lower average firing
 rates.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename UpdatedFigures/selectivity_vs_average_firing_rate.eps
	scale 16

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Individual neuronal object selectivity and average firing rate for a population
 of 674 neurons over a test set of 806 objects.
 Average firing rate decreases as the selectivity of a neuron increases.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:object_selectivity_vs_firing_rate"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Position Tuning
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Distribution-of-rf-centers"

\end_inset

 plots the distribution of receptive field centers of a generated population
 of IT neuron alongside measured receptive field centers found in 
\begin_inset CommandInset citation
LatexCommand citet
key "op2000"

\end_inset

.
 Normal distributions with means 1.82
\begin_inset Formula $\textdegree$
\end_inset

, and 0.62
\begin_inset Formula $\textdegree$
\end_inset

 and standard deviations 2.02
\begin_inset Formula $\textdegree$
\end_inset

, 2.12
\begin_inset Formula $\textdegree$
\end_inset

 were used to model the x and y coordinates of the receptive field centers
 respectively (see 
\begin_inset CommandInset ref
LatexCommand nameref
reference "sec:Methods"

\end_inset

).
 All angles are in degrees of eccentricity.
 For the same IT neuron population, figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Postion-tolerance-verses-selectivity"

\end_inset

 plots position tolerances verses activity fraction selectivity.
 As observed by 
\begin_inset CommandInset citation
LatexCommand citet
key "zoccolan2007"

\end_inset

 more selective neurons have lower tolerances to positional changes.
 
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Postion-tolerance-verses-selectivity"

\end_inset

 also shows that our model does not generate neurons as highly selective
 as those recorded by 
\begin_inset CommandInset citation
LatexCommand citet
key "zoccolan2007"

\end_inset

 when measured using activity fractions.
 Selectivity was measured over responses to test stimuli only, while in
 our model activity fraction selectivity is calculated over the entire gamma
 selectivity distribution.
 It is likely the some of the recorded activity fraction selectivity were
 exaggerated simply because the test stimuli set did not include many of
 the objects a recorded neuron was responsive to.
 Because we use the activity fraction selectivity to generate the position
 tolerance of IT neurons, our model may not generate some of the observed
 smaller receptive fields.
 Still, the critical observation that position tolerance decreases with
 selectivity is observable.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename UpdatedFigures/rf_centers_distribution.eps
	scale 15

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Distribution of spatial receptive field centers.
 Original data from figure 6 of 
\begin_inset CommandInset citation
LatexCommand citet
key "op2000"

\end_inset

.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Distribution-of-rf-centers"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename UpdatedFigures/position_tolerance_vs_selectivity.eps
	scale 15

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Postion tolerance verses activity fraction selectivity.
 Straight lines are best linear regression fits of the data found using
 LSE.
 Original data from figure 4a of 
\begin_inset CommandInset citation
LatexCommand citet
key "zoccolan2007"

\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "fig:Postion-tolerance-verses-selectivity"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Size Tuning
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-size-tuning-profiles"

\end_inset

 shows the three types of IT neuronal size tuning profiles seen in 
\begin_inset CommandInset citation
LatexCommand citet
key "ito1995"

\end_inset

, along with fits generated using our size tuning model.
 Most neurons displayed narrow size tuning with bandwidths less than two
 octaves.
 A few neurons showed much broader tuning with size bandwidths greater than
 five octaves.
 The third type of tuning curve, where maximum responses was seen for the
 largest stimulus sizes tested, is similar to the narrowly tuning profile
 but for which the higher half magnitude response size was not determined.
 We plot tuning profiles only up to the maximum stimulus size of a model
 neuron.
 Responses to larger stimuli decrease gradually following the curvature
 of the underlying lognormal distribution.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename UpdatedFigures/size_tuning_profiles.eps
	scale 15

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Example size tuning profiles.
 Original data from figures 3, 4 and 5 of 
\begin_inset CommandInset citation
LatexCommand citet
key "ito1995"

\end_inset

.
 (A) narrow size bandwidth, (B) wide size bandwith and (C) responded well
 to only the largest stimulus size.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Example-size-tuning-profiles"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Orientation Tuning 
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-Rotation-Tuning"

\end_inset

 shows the three types of rotation tuning profiles seen in 
\begin_inset CommandInset citation
LatexCommand citet
key "logothetis1995"

\end_inset

 fitted with our orientation tuning model.
 Pseudo mirror symmetric profiles, characterized by two 180 degree separated
 peaks, are generated by objects with symmetry frequencies of two.
 Mirror symmetric profiles, such as those seen in 
\begin_inset CommandInset citation
LatexCommand citet
key "freiwald2010"

\end_inset

, can also be generated using the mirror symmetric component of our model.
 Objects will large symmetry frequencies converge towards a view invariant
 profile regardless of the preferred view and rotational tolerance of individual
 neurons.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename UpdatedFigures/rotation_tuning_profiles.eps
	scale 17

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Types of IT rotation tuning profiles.
 Original data from figure 5 of 
\begin_inset CommandInset citation
LatexCommand citet
key "logothetis1995"

\end_inset

.
 First column, single preferred view.
 Second column, pseudo mirror symmetric with preferred orientations separated
 by 180
\begin_inset Formula $\textdegree$
\end_inset

 degrees.
 Third column view invariant profile.
\begin_inset CommandInset label
LatexCommand label
name "fig:Example-Rotation-Tuning"

\end_inset

 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Occlusion Tuning
\end_layout

\begin_layout Standard
In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Sample-occlusion-tuning"

\end_inset

a we fit our occlusion model to the normalized responses of object ranked
 1 in the moving occluder data of 
\begin_inset CommandInset citation
LatexCommand cite
key "kovacs1995"

\end_inset

 and found the best fit parameters for tuning along combined visibility
 axis, 
\begin_inset Formula $w_{c}$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

.
 Given a neurons preference for diagnostic parts, 
\begin_inset Formula $R$
\end_inset

, we find a set of 
\begin_inset Formula $w_{d}$
\end_inset

 and 
\begin_inset Formula $w_{nd}$
\end_inset

 and construct its complete 3D tuning profile, figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Sample-occlusion-tuning"

\end_inset

b.
 Tuning along the combined visibilities axis is also plotted using 
\begin_inset Formula $v_{c}=\sqrt{(v_{d}^{2}+v_{nd}^{2})/2}$
\end_inset

 to relate visibilities.
 This allows 
\begin_inset Formula $v_{c}$
\end_inset

, 
\begin_inset Formula $v_{d}$
\end_inset

 and 
\begin_inset Formula $v_{nd}$
\end_inset

 to range between 
\begin_inset Formula $(0,1)$
\end_inset

.
 Mean 
\begin_inset Formula $R$
\end_inset

 was found to be 0.1462 and it gets exponentially harder to generate high
 R values.
 As noted by 
\begin_inset CommandInset citation
LatexCommand citet
key "nielsen2006"

\end_inset

, most IT neurons are sensitive to both diagnosticity and visibility dimensions
 and fire maximally when both diagnostic and nondiagnostic parts are fully
 visible.
 Consequently, they may be encoding object occlusion levels in their responses
 which may potentially be extracted from population activity.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename UpdatedFigures/occlusion_tuning_profile.eps
	scale 14

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Sample occlusion tuning profile.
 Left tuning along the combined visibilities axis.
 Right, complete two input logistic profile.
 Original data from figure 8 of 
\begin_inset CommandInset citation
LatexCommand citet
key "kovacs1995"

\end_inset

.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Sample-occlusion-tuning"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section*
Discussion
\end_layout

\begin_layout Standard

\emph on
Short Recap
\end_layout

\begin_layout Standard
We modeled the responses of IT neurons to a number of different factors
 including object selectivity, position, size, rotation, occlusion, clutter
 and their dynamic responses.
 We showed that our model can fit multiple recorded individual tuning profiles
 with reasonable parameters.
 Furthermore, we developed models for the distributions of their parameters
 and show that our integrated model can successfully reproduced several
 observed population characteristics as well.
 We developed a mechanism for extracting object ground truth from complex
 visual environments and show how our integrated model can produced realistic
 firing patterns out of them.
\end_layout

\begin_layout Standard

\emph on
Assumption of independent tuning curves
\end_layout

\begin_layout Standard

\emph on
Connectivity between neurons & structural organization of IT cortex
\end_layout

\begin_layout Standard

\emph on
Comparison with other IT models
\end_layout

\begin_layout Standard

\emph on
How many tuning curves to include before our IT cortex model can be considered
 close to the real IT Cortex? 
\emph default
Obviously, we cannot model all IT attributes found in the literature.
 Neither have all attributes of IT neurons been identified nor completely
 understood.
 
\begin_inset CommandInset citation
LatexCommand cite
key "lehky2014"

\end_inset

 estimated the intrinsic dimensionality of the IT cortex to be 93
\begin_inset space ~
\end_inset


\begin_inset Formula $\pm\,11$
\end_inset

.
 Most papers use stimuli sample sizes and record from populations smaller
 than this dimensionally and as a result do not capture sufficient data
 to completely characterize each attribute.
 However we believe the current set is comprehensive enough to allow realistic
 enough representation of IT neurons.
 In the future, we plan on including more attributes.
 The model is designed to easily incorporate new attributes.
\end_layout

\begin_layout Standard

\emph on
Why is this work useful?
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "bibtex"
options "plainnat"

\end_inset


\end_layout

\end_body
\end_document
